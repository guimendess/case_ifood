{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60659ab0-d02d-4f25-aafa-40b92e940b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta-spark==1.1.0\n",
      "  Downloading delta_spark-1.1.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from delta-spark==1.1.0) (4.11.4)\n",
      "Collecting pyspark<3.3.0,>=3.2.0\n",
      "  Downloading pyspark-3.2.4.tar.gz (281.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.5/281.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=1.0.0->delta-spark==1.1.0) (3.9.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.10/site-packages (from pyspark<3.3.0,>=3.2.0->delta-spark==1.1.0) (0.10.9.5)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.4-py2.py3-none-any.whl size=282040922 sha256=1b0c5abedf166228d3fe2f814de6eec1443664b316abe6d1a5eb049c33c11a98\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/bf/f6/18/acaa11d057c23e749eea1773c9516d58fbc8ddcdd1492ad3fd\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark, delta-spark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.3.0\n",
      "    Can't uninstall 'pyspark'. No files were found to uninstall.\n",
      "  Attempting uninstall: delta-spark\n",
      "    Found existing installation: delta-spark 2.3.0\n",
      "    Uninstalling delta-spark-2.3.0:\n",
      "      Successfully uninstalled delta-spark-2.3.0\n",
      "Successfully installed delta-spark-1.1.0 pyspark-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-spark==1.1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7712598-d065-4255-bc4f-1e41d99c4d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delta-spark in /opt/conda/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from delta-spark) (4.11.4)\n",
      "Collecting pyspark<3.3.0,>=3.2.0\n",
      "  Using cached pyspark-3.2.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.9.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.10/site-packages (from pyspark<3.3.0,>=3.2.0->delta-spark) (0.10.9.5)\n",
      "Installing collected packages: pyspark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.3.0\n",
      "    Can't uninstall 'pyspark'. No files were found to uninstall.\n",
      "Successfully installed pyspark-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d76451f-85ba-41bf-98af-b5dc5c49c702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from IPython.core.display import HTML\n",
    "from pyspark.sql import functions as F\n",
    "from delta import *\n",
    "\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .config(\"spark.jars\",\"\"\"/home/jovyan/jars/aws-java-sdk-core-1.11.534.jar,\n",
    "                                 /home/jovyan/jars/aws-java-sdk-dynamodb-1.11.534.jar,\n",
    "                                 /home/jovyan/jars/aws-java-sdk-s3-1.11.534.jar,\n",
    "                                 /home/jovyan/jars/hadoop-aws-3.2.2.jar,\n",
    "                                 /home/jovyan/jars/postgresql-42.3.3.jar\"\"\")\n",
    "         .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "         .config(\"spark.hadoop.fs.s3a.access.key\", \"ZiVYnHFvBt2HumCAXmQG\")\n",
    "         .config(\"spark.hadoop.fs.s3a.secret.key\", \"oVNgBEZ5PISLiwF25fJkoazHGXRFB7mJeiBcVygd\")\n",
    "         .config(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "         .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "         .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13ed836c-1648-44be-bc21-60e869ddffab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+------------+--------------------+---------------------+\n",
      "|VendorID|passenger_count|total_amount|tpep_pickup_datetime|tpep_dropoff_datetime|\n",
      "+--------+---------------+------------+--------------------+---------------------+\n",
      "|       2|              1|        14.3| 2023-01-01 00:32:10|  2023-01-01 00:40:36|\n",
      "|       2|              1|        16.9| 2023-01-01 00:55:08|  2023-01-01 01:01:27|\n",
      "|       2|              1|        34.9| 2023-01-01 00:25:04|  2023-01-01 00:37:49|\n",
      "|       1|              0|       20.85| 2023-01-01 00:03:48|  2023-01-01 00:13:25|\n",
      "|       2|              1|       19.68| 2023-01-01 00:10:29|  2023-01-01 00:21:19|\n",
      "|       2|              1|        27.8| 2023-01-01 00:50:34|  2023-01-01 01:02:52|\n",
      "|       2|              1|       20.52| 2023-01-01 00:09:22|  2023-01-01 00:19:49|\n",
      "|       2|              1|       64.44| 2023-01-01 00:27:12|  2023-01-01 00:49:56|\n",
      "|       2|              1|       28.38| 2023-01-01 00:21:44|  2023-01-01 00:36:40|\n",
      "|       2|              1|        19.9| 2023-01-01 00:39:42|  2023-01-01 00:50:36|\n",
      "|       2|              1|       19.68| 2023-01-01 00:53:01|  2023-01-01 01:01:45|\n",
      "|       1|              4|       46.55| 2023-01-01 00:43:37|  2023-01-01 01:17:18|\n",
      "|       2|              1|       37.32| 2023-01-01 00:34:44|  2023-01-01 01:04:25|\n",
      "|       2|              2|       66.31| 2023-01-01 00:09:29|  2023-01-01 00:29:23|\n",
      "|       2|              1|       24.24| 2023-01-01 00:33:53|  2023-01-01 00:49:15|\n",
      "|       2|              1|       16.25| 2023-01-01 00:13:04|  2023-01-01 00:22:10|\n",
      "|       2|              1|       29.76| 2023-01-01 00:45:11|  2023-01-01 01:07:39|\n",
      "|       1|              1|        29.5| 2023-01-01 00:04:33|  2023-01-01 00:19:22|\n",
      "|       1|              3|        13.6| 2023-01-01 00:03:36|  2023-01-01 00:09:36|\n",
      "|       1|              2|        20.6| 2023-01-01 00:15:23|  2023-01-01 00:29:41|\n",
      "+--------+---------------+------------+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+---------------+------------+--------------------+---------------------+\n",
      "|VendorID|passenger_count|total_amount|tpep_pickup_datetime|tpep_dropoff_datetime|\n",
      "+--------+---------------+------------+--------------------+---------------------+\n",
      "|       1|              2|         9.4| 2023-02-01 00:32:53|  2023-02-01 00:34:34|\n",
      "|       2|              1|        -5.5| 2023-02-01 00:35:16|  2023-02-01 00:35:30|\n",
      "|       2|              1|         5.5| 2023-02-01 00:35:16|  2023-02-01 00:35:30|\n",
      "|       1|              0|       74.65| 2023-02-01 00:29:33|  2023-02-01 01:01:38|\n",
      "|       2|              1|        25.3| 2023-02-01 00:12:28|  2023-02-01 00:25:46|\n",
      "|       1|              1|       32.25| 2023-02-01 00:52:40|  2023-02-01 01:07:18|\n",
      "|       1|              1|        50.0| 2023-02-01 00:12:39|  2023-02-01 00:40:36|\n",
      "|       1|              1|       14.64| 2023-02-01 00:56:53|  2023-02-01 01:00:37|\n",
      "|       2|              1|       44.12| 2023-02-01 00:20:40|  2023-02-01 00:33:56|\n",
      "|       2|              1|       12.42| 2023-02-01 00:33:51|  2023-02-01 00:37:34|\n",
      "|       2|              1|       14.64| 2023-02-01 01:00:45|  2023-02-01 01:06:00|\n",
      "|       2|              1|        16.0| 2023-02-01 00:10:48|  2023-02-01 00:18:09|\n",
      "|       2|              1|        30.3| 2023-02-01 00:00:28|  2023-02-01 00:12:54|\n",
      "|       2|              1|       25.56| 2023-02-01 00:35:06|  2023-02-01 00:45:47|\n",
      "|       2|              5|       15.25| 2023-02-01 00:06:00|  2023-02-01 00:10:31|\n",
      "|       2|              5|       20.52| 2023-02-01 00:31:02|  2023-02-01 00:41:26|\n",
      "|       2|              5|        12.2| 2023-02-01 00:47:56|  2023-02-01 00:53:02|\n",
      "|       1|              1|       98.15| 2023-02-01 00:01:09|  2023-02-01 00:51:49|\n",
      "|       1|              1|        83.8| 2023-02-01 00:41:47|  2023-02-01 01:14:14|\n",
      "|       2|              2|        26.9| 2023-02-01 00:22:16|  2023-02-01 00:40:34|\n",
      "+--------+---------------+------------+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+---------------+------------+--------------------+---------------------+\n",
      "|VendorID|passenger_count|total_amount|tpep_pickup_datetime|tpep_dropoff_datetime|\n",
      "+--------+---------------+------------+--------------------+---------------------+\n",
      "|       2|              1|        11.1| 2023-03-01 00:06:43|  2023-03-01 00:16:43|\n",
      "|       2|              2|       76.49| 2023-03-01 00:08:25|  2023-03-01 00:39:30|\n",
      "|       1|              0|       28.05| 2023-03-01 00:15:04|  2023-03-01 00:29:26|\n",
      "|       1|              1|        24.7| 2023-03-01 00:49:37|  2023-03-01 01:01:05|\n",
      "|       2|              1|       14.64| 2023-03-01 00:08:04|  2023-03-01 00:11:06|\n",
      "|       1|              1|        18.0| 2023-03-01 00:09:09|  2023-03-01 00:17:34|\n",
      "|       1|              1|        20.5| 2023-03-01 00:32:21|  2023-03-01 00:42:08|\n",
      "|       1|              1|        15.7| 2023-03-01 00:45:12|  2023-03-01 00:52:37|\n",
      "|       1|              1|        40.4| 2023-03-01 00:19:43|  2023-03-01 00:39:37|\n",
      "|       2|              1|        22.2| 2023-03-01 00:08:42|  2023-03-01 00:18:45|\n",
      "|       2|              1|        15.3| 2023-03-01 00:48:06|  2023-03-01 00:57:15|\n",
      "|       1|              1|       12.95| 2023-03-01 00:10:23|  2023-03-01 00:13:06|\n",
      "|       2|              5|        15.0| 2023-03-01 00:22:22|  2023-03-01 00:32:01|\n",
      "|       1|              1|        25.1| 2023-03-01 00:01:34|  2023-03-01 00:16:18|\n",
      "|       2|              1|       17.16| 2023-03-01 00:09:42|  2023-03-01 00:15:50|\n",
      "|       2|              1|       12.96| 2023-03-01 00:45:09|  2023-03-01 00:49:08|\n",
      "|       2|              1|       18.35| 2023-03-01 00:55:38|  2023-03-01 01:06:05|\n",
      "|       1|              1|        13.8| 2023-03-01 00:10:30|  2023-03-01 00:13:49|\n",
      "|       1|              1|        17.1| 2023-03-01 00:20:49|  2023-03-01 00:30:31|\n",
      "|       2|              2|        93.8| 2023-03-01 00:29:18|  2023-03-01 01:06:01|\n",
      "+--------+---------------+------------+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+---------------+------------+--------------------+---------------------+\n",
      "|VendorID|passenger_count|total_amount|tpep_pickup_datetime|tpep_dropoff_datetime|\n",
      "+--------+---------------+------------+--------------------+---------------------+\n",
      "|       1|              2|        39.9| 2023-04-01 00:14:49|  2023-04-01 00:45:01|\n",
      "|       2|              1|        81.8| 2023-04-01 00:00:24|  2023-04-01 00:56:19|\n",
      "|       1|              2|        18.4| 2023-04-01 00:03:50|  2023-04-01 00:14:42|\n",
      "|       1|              1|        16.0| 2023-04-01 00:53:18|  2023-04-01 01:01:28|\n",
      "|       2|              2|        17.4| 2023-04-01 00:07:00|  2023-04-01 00:17:16|\n",
      "|       1|              6|       17.85| 2023-04-01 00:08:59|  2023-04-01 00:15:39|\n",
      "|       2|              1|       61.92| 2023-04-01 00:27:52|  2023-04-01 00:43:07|\n",
      "|       2|              1|       33.62| 2023-04-01 00:48:38|  2023-04-01 01:08:37|\n",
      "|       1|              0|        48.8| 2023-04-01 00:22:28|  2023-04-01 00:34:29|\n",
      "|       2|              1|       17.16| 2023-04-01 00:27:06|  2023-04-01 00:34:06|\n",
      "|       2|              1|        13.9| 2023-04-01 00:34:42|  2023-04-01 00:41:30|\n",
      "|       2|              1|        14.6| 2023-04-01 00:47:38|  2023-04-01 00:53:55|\n",
      "|       2|              1|       18.86| 2023-04-01 00:59:10|  2023-04-01 01:08:55|\n",
      "|       2|              2|       44.04| 2023-04-01 00:19:36|  2023-04-01 00:49:31|\n",
      "|       1|              2|        36.1| 2023-04-01 00:04:59|  2023-04-01 00:30:10|\n",
      "|       2|              1|       27.24| 2023-04-01 00:16:49|  2023-04-01 00:33:01|\n",
      "|       2|              1|        26.0| 2023-04-01 00:11:21|  2023-04-01 00:24:43|\n",
      "|       2|              1|       24.72| 2023-04-01 00:38:58|  2023-04-01 00:52:20|\n",
      "|       1|              2|        12.2| 2023-04-01 00:03:43|  2023-04-01 00:09:16|\n",
      "|       1|              1|        17.4| 2023-04-01 00:12:17|  2023-04-01 00:20:34|\n",
      "+--------+---------------+------------+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+---------------+------------+--------------------+---------------------+\n",
      "|VendorID|passenger_count|total_amount|tpep_pickup_datetime|tpep_dropoff_datetime|\n",
      "+--------+---------------+------------+--------------------+---------------------+\n",
      "|       1|              0|       51.65| 2023-05-01 00:33:13|  2023-05-01 00:53:01|\n",
      "|       1|              2|       57.15| 2023-05-01 00:42:49|  2023-05-01 01:11:18|\n",
      "|       1|              2|        64.2| 2023-05-01 00:56:34|  2023-05-01 01:13:39|\n",
      "|       2|              1|       47.09| 2023-05-01 00:00:52|  2023-05-01 00:20:12|\n",
      "|       1|              0|       59.15| 2023-05-01 00:05:50|  2023-05-01 00:19:41|\n",
      "|       1|              0|        69.0| 2023-05-01 00:42:54|  2023-05-01 01:04:49|\n",
      "|       2|              1|       64.56| 2023-05-01 00:50:34|  2023-05-01 01:12:09|\n",
      "|       1|              1|       14.35| 2023-05-01 00:13:58|  2023-05-01 00:18:10|\n",
      "|       2|              1|        17.1| 2023-04-30 23:48:31|  2023-04-30 23:57:35|\n",
      "|       2|              1|        19.9| 2023-05-01 00:28:47|  2023-05-01 00:39:33|\n",
      "|       2|              1|       78.67| 2023-05-01 00:18:49|  2023-05-01 00:43:41|\n",
      "|       1|              1|        53.5| 2023-05-01 00:52:48|  2023-05-01 01:17:35|\n",
      "|       2|              1|       14.64| 2023-05-01 00:55:22|  2023-05-01 00:58:20|\n",
      "|       2|              1|        19.1| 2023-05-01 00:28:12|  2023-05-01 00:36:09|\n",
      "|       2|              2|       76.89| 2023-05-01 00:04:27|  2023-05-01 00:26:38|\n",
      "|       1|              1|         9.4| 2023-05-01 00:07:05|  2023-05-01 00:08:45|\n",
      "|       1|              1|       17.25| 2023-05-01 00:30:56|  2023-05-01 00:36:33|\n",
      "|       2|              1|       83.88| 2023-05-01 00:47:34|  2023-05-01 01:17:55|\n",
      "|       2|              1|        10.8| 2023-05-01 00:53:26|  2023-05-01 00:57:47|\n",
      "|       2|              1|       69.55| 2023-05-01 00:34:15|  2023-05-01 00:51:43|\n",
      "+--------+---------------+------------+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definindo o diretório de origem e o diretório de destino\n",
    "input_path_format = \"s3a://bronze/yellow_taxi_files/yellow_tripdata_2023-{month:02d}.parquet\"\n",
    "output_path = \"s3a://silver/prd_yellow_taxi_table\"\n",
    "\n",
    "# Loop para iterar sobre os meses de 01 a 05\n",
    "for month in range(1, 6):\n",
    "    # Formatando o caminho do arquivo para o mês atual\n",
    "    parquet_path = input_path_format.format(month=month)\n",
    "\n",
    "    # Carregar os arquivos Parquet\n",
    "    df = spark.read.parquet(parquet_path)\n",
    "\n",
    "    # Caso haja discrepâncias de tipo entre os arquivos, faça o cast necessário\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"VendorID\", F.col(\"VendorID\").cast(IntegerType()))\n",
    "        .withColumn(\"tpep_pickup_datetime\", F.col(\"tpep_pickup_datetime\").cast(TimestampType()))\n",
    "        .withColumn(\"tpep_dropoff_datetime\", F.col(\"tpep_dropoff_datetime\").cast(TimestampType()))\n",
    "        .withColumn(\"passenger_count\", F.col(\"passenger_count\").cast(IntegerType()))\n",
    "        .withColumn(\"trip_distance\", F.col(\"trip_distance\").cast(DoubleType()))\n",
    "        .withColumn(\"RatecodeID\", F.col(\"RatecodeID\").cast(IntegerType()))\n",
    "        .withColumn(\"store_and_fwd_flag\", F.col(\"store_and_fwd_flag\").cast(StringType()))\n",
    "        .withColumn(\"PULocationID\", F.col(\"PULocationID\").cast(IntegerType()))\n",
    "        .withColumn(\"DOLocationID\", F.col(\"DOLocationID\").cast(IntegerType()))\n",
    "        .withColumn(\"payment_type\", F.col(\"payment_type\").cast(LongType()))\n",
    "        .withColumn(\"fare_amount\", F.col(\"fare_amount\").cast(DoubleType()))\n",
    "        .withColumn(\"extra\", F.col(\"extra\").cast(DoubleType()))\n",
    "        .withColumn(\"mta_tax\", F.col(\"mta_tax\").cast(DoubleType()))\n",
    "        .withColumn(\"tip_amount\", F.col(\"tip_amount\").cast(DoubleType()))\n",
    "        .withColumn(\"tolls_amount\", F.col(\"tolls_amount\").cast(DoubleType()))\n",
    "        .withColumn(\"improvement_surcharge\", F.col(\"improvement_surcharge\").cast(DoubleType()))\n",
    "        .withColumn(\"total_amount\", F.col(\"total_amount\").cast(DoubleType()))\n",
    "        .withColumn(\"congestion_surcharge\", F.col(\"congestion_surcharge\").cast(DoubleType()))\n",
    "        .withColumn(\"airport_fee\", F.col(\"airport_fee\").cast(DoubleType()))\n",
    "    )\n",
    "\n",
    "    # Selecionando apenas as colunas necessárias\n",
    "    df = (\n",
    "        df.select(\n",
    "        'VendorID',\n",
    "        'passenger_count',\n",
    "        'total_amount',\n",
    "        'tpep_pickup_datetime',\n",
    "        'tpep_dropoff_datetime'\n",
    "        )\n",
    "        .filter(\n",
    "                (F.year(df[\"tpep_pickup_datetime\"]) == 2023) & \n",
    "                (F.month(df[\"tpep_pickup_datetime\"]).between(1, 5))\n",
    "            )\n",
    "      )\n",
    "\n",
    "    # Exibindo o DataFrame para conferir\n",
    "    df.show()\n",
    "\n",
    "    # Salvando os dados no formato Delta no path final com append\n",
    "    (\n",
    "        df\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('append')  # Usando append para adicionar ao Delta Lake\n",
    "        .save(output_path)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd2e6e70-3d4e-4edf-bb63-87953ad3edce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+------------+--------------------+---------------------+\n",
      "|VendorID|passenger_count|total_amount|tpep_pickup_datetime|tpep_dropoff_datetime|\n",
      "+--------+---------------+------------+--------------------+---------------------+\n",
      "|       1|              2|         9.4| 2023-02-01 00:32:53|  2023-02-01 00:34:34|\n",
      "|       2|              1|        -5.5| 2023-02-01 00:35:16|  2023-02-01 00:35:30|\n",
      "|       2|              1|         5.5| 2023-02-01 00:35:16|  2023-02-01 00:35:30|\n",
      "|       1|              0|       74.65| 2023-02-01 00:29:33|  2023-02-01 01:01:38|\n",
      "|       2|              1|        25.3| 2023-02-01 00:12:28|  2023-02-01 00:25:46|\n",
      "|       1|              1|       32.25| 2023-02-01 00:52:40|  2023-02-01 01:07:18|\n",
      "|       1|              1|        50.0| 2023-02-01 00:12:39|  2023-02-01 00:40:36|\n",
      "|       1|              1|       14.64| 2023-02-01 00:56:53|  2023-02-01 01:00:37|\n",
      "|       2|              1|       44.12| 2023-02-01 00:20:40|  2023-02-01 00:33:56|\n",
      "|       2|              1|       12.42| 2023-02-01 00:33:51|  2023-02-01 00:37:34|\n",
      "|       2|              1|       14.64| 2023-02-01 01:00:45|  2023-02-01 01:06:00|\n",
      "|       2|              1|        16.0| 2023-02-01 00:10:48|  2023-02-01 00:18:09|\n",
      "|       2|              1|        30.3| 2023-02-01 00:00:28|  2023-02-01 00:12:54|\n",
      "|       2|              1|       25.56| 2023-02-01 00:35:06|  2023-02-01 00:45:47|\n",
      "|       2|              5|       15.25| 2023-02-01 00:06:00|  2023-02-01 00:10:31|\n",
      "|       2|              5|       20.52| 2023-02-01 00:31:02|  2023-02-01 00:41:26|\n",
      "|       2|              5|        12.2| 2023-02-01 00:47:56|  2023-02-01 00:53:02|\n",
      "|       1|              1|       98.15| 2023-02-01 00:01:09|  2023-02-01 00:51:49|\n",
      "|       1|              1|        83.8| 2023-02-01 00:41:47|  2023-02-01 01:14:14|\n",
      "|       2|              2|        26.9| 2023-02-01 00:22:16|  2023-02-01 00:40:34|\n",
      "+--------+---------------+------------+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from delta import *\n",
    "\n",
    "df_delta = spark.read.format(\"delta\").load('s3a://silver/prd_yellow_taxi_table')\n",
    "\n",
    "df_delta.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dc49774-d62a-4d45-81e6-3e5c8651b9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_delta.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20bb2ae1-6b82-44c1-9371-1f38693465de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------------+\n",
      "|year|month|total_amount_sum|\n",
      "+----+-----+----------------+\n",
      "|2023|    1|   82,863,594.11|\n",
      "|2023|    2|   78,381,600.43|\n",
      "|2023|    3|   94,634,027.66|\n",
      "|2023|    4|   92,956,843.11|\n",
      "|2023|    5|  101,765,282.92|\n",
      "+----+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = (\n",
    "    df_delta.groupBy(F.year(\"tpep_pickup_datetime\").alias(\"year\"), F.month(\"tpep_pickup_datetime\").alias(\"month\"))\n",
    "    .agg(F.sum(\"total_amount\").alias(\"total_amount_sum\"))\n",
    "    .withColumn(\"total_amount_sum\", F.format_number(\"total_amount_sum\", 2))\n",
    "    .orderBy(\"year\", \"month\")\n",
    ")\n",
    "\n",
    "# Exibindo o resultado\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
